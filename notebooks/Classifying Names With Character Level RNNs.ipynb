{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Names with a Character Level RNN\n",
    "\n",
    "In this tutorial we try to predict the nationality of a name by processing it through a bidirectional LSTM one character at a time. We have 18 input files, each corresponding to a different nationality. We will store the names in a dictionary of the form `{'Nationality': [name1, name2, ...], ...}`. The [`glob` module](https://docs.python.org/3/library/glob.html) is more convenient than `os.listdir` for this purpsose, as it returns the full path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/names/German.txt', '../data/names/Arabic.txt', '../data/names/Vietnamese.txt', '../data/names/Dutch.txt', '../data/names/Polish.txt', '../data/names/Portuguese.txt', '../data/names/Scottish.txt', '../data/names/Korean.txt', '../data/names/Irish.txt', '../data/names/Russian.txt', '../data/names/Czech.txt', '../data/names/Greek.txt', '../data/names/Italian.txt', '../data/names/Spanish.txt', '../data/names/French.txt', '../data/names/Japanese.txt', '../data/names/English.txt', '../data/names/Chinese.txt']\n"
     ]
    }
   ],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import string\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "file_list = glob.glob('../data/names/*.txt')\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = {}\n",
    "for file_name in file_list:\n",
    "    nationality = os.path.basename(os.path.splitext(file_name)[0])\n",
    "    name_list = []\n",
    "    for line in open(file_name, 'r'):\n",
    "        name_list.append(line.rstrip())\n",
    "    category_lines[nationality] = name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the names contain accents and other non-ASCII characters that can make things complicated. We may convert all the characters to plain ASCII, as shown in the vignette, but our task would probably be easier if we could account for the non-ASCII characters. Let's try this approach! We read each file in turn, and for each file we extract the set of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_characters(filename):\n",
    "    character_sets = []\n",
    "    for name in open(filename, 'r'):\n",
    "        character_sets.append(set(name.lower()))\n",
    "    unique_characters = reduce(set.union, character_sets)\n",
    "    return unique_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with the first element of `file_list`, which contains the German names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g', 'ß', 'ä', 'w', 'i', 'k', 'd', 'n', 'l', 'e', 'ü', 'x', 'q', 'o', 'ö', ' ', 'y', '\\n', 'c', 'b', 'a', 'h', 'f', 'z', 's', 'm', 't', 'r', 'v', 'u', 'j', 'p'}\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "unique_german = get_unique_characters(file_list[0])\n",
    "print(unique_german)\n",
    "print(len(unique_german))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this method on all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\", ',', 'g', 'ß', 'ä', 'w', 'i', 'k', 'ż', '-', '/', '1', 'á', 'e', 'õ', 'ł', 'ö', ' ', 'ñ', '\\n', 'b', 'ã', 'a', 'é', ':', 's', 'm', 't', '\\xa0', 'u', 'p', 'ú', 'ą', 'à', 'ì', 'd', 'ń', 'n', 'ê', 'l', 'ò', 'ü', 'x', 'q', 'ś', 'o', 'í', 'ç', 'ù', 'y', 'c', 'h', 'f', 'z', 'è', 'r', 'v', 'j', 'ó'}\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "all_unique_characters = [get_unique_characters(f) for f in file_list]\n",
    "all_unique_characters = reduce(set.union, all_unique_characters)\n",
    "print(all_unique_characters)\n",
    "print(len(all_unique_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us all the characters appearing in the files. We can turn this into a dictionary to later create one-hot encodings of the individual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\": 0, ',': 1, 'g': 2, 'ß': 3, 'ä': 4, 'w': 5, 'i': 6, 'k': 7, 'ż': 8, '-': 9, '/': 10, '1': 11, 'á': 12, 'e': 13, 'õ': 14, 'ł': 15, 'ö': 16, ' ': 17, 'ñ': 18, '\\n': 19, 'b': 20, 'ã': 21, 'a': 22, 'é': 23, ':': 24, 's': 25, 'm': 26, 't': 27, '\\xa0': 28, 'u': 29, 'p': 30, 'ú': 31, 'ą': 32, 'à': 33, 'ì': 34, 'd': 35, 'ń': 36, 'n': 37, 'ê': 38, 'l': 39, 'ò': 40, 'ü': 41, 'x': 42, 'q': 43, 'ś': 44, 'o': 45, 'í': 46, 'ç': 47, 'ù': 48, 'y': 49, 'c': 50, 'h': 51, 'f': 52, 'z': 53, 'è': 54, 'r': 55, 'v': 56, 'j': 57, 'ó': 58}\n"
     ]
    }
   ],
   "source": [
    "character_dict = {char: ix for ix, char in enumerate(all_unique_characters)}\n",
    "print(character_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
