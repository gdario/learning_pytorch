{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive Into Torch Tensors\n",
    "\n",
    "This notebook goes a bit more into the details of `torch.Tensor` objects.\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Each tensor has a type and a shape. Tensors are associated with `Storage` objects. A tensor is not a `Storage` object, but it contains one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.randn(2, 2)\n",
    "print(torch.is_tensor(x))  # True\n",
    "print(torch.is_storage(x)) # False\n",
    "print(torch.is_storage(x.storage())) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type\n",
    "\n",
    "The type of a tensor is generally decided when the tensor is intantiated. One can add flexibility by using the `dtype` attribute as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "# If CUDA is available, one can use instead:\n",
    "# dtype = torch.cuda.FloatTensor\n",
    "\n",
    "x = torch.Tensor(2, 2).type(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing, Slicing, Joining, Mutating Ops\n",
    "\n",
    "#### torch.cat\n",
    "\n",
    "`torch.cat` concatenates a sequence of tensors along a given `dim`. Optionally one can specify the output tensor `out`. Remember:\n",
    "- `dim=0` is equivalent to R's `rowbind`.\n",
    "- `dim=1` is equivalent to R's `colbind`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = torch.randn(2, 3), torch.randn(4, 3), torch.randn(2, 4)\n",
    "\n",
    "print(torch.cat((x1, x2), dim=0).size()) # concatenate by row (same n. of columns)\n",
    "print(torch.cat((x1, x3), dim=1).size()) # concatenate by column (same n. or rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.chunk\n",
    "\n",
    "`torch.chunk` splits a tensor into a list of a given number of sub-tensors (\"chunks\") along a given axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 0.1816 -0.2602 -0.0569\n",
      " 0.6215  0.4222  0.6585\n",
      "[torch.FloatTensor of size 2x3]\n",
      ", \n",
      "-1.3553 -0.2051 -0.2057\n",
      "-0.2135  0.5906 -0.3909\n",
      "[torch.FloatTensor of size 2x3]\n",
      ", \n",
      " 1.2769  0.1044 -1.4429\n",
      " 0.6632 -0.0868  0.5538\n",
      "[torch.FloatTensor of size 2x3]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x12 = torch.cat((x1, x2), dim=0)\n",
    "print(torch.chunk(x12, chunks=3, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.gather\n",
    "\n",
    "**TODO** how does it work??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.index_select\n",
    "\n",
    "`torch.index_select` selects the element of a tensor based on a `torch.LongTensor` of indices and a `dim`. For exaple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.5316 -0.5159  0.9304 -0.1629\n",
      " 1.2964 -1.1541 -0.1048 -1.0506\n",
      "[torch.FloatTensor of size 2x4]\n",
      "\n",
      "\n",
      "-0.5316  0.9304\n",
      "-1.3817  1.3097\n",
      " 1.2964 -0.1048\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "indices = torch.LongTensor([0, 2])\n",
    "print(torch.index_select(x, dim=0, index=indices)) # Select *rows* 0 and 2\n",
    "print(torch.index_select(x, dim=1, index=indices)) # Select *columns* 0 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.masked_select\n",
    "\n",
    "Given a tensor, we can create a boolean mask of class `torch.ByteTensor` and use this mask to select some elements that will always be returned as a 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 3, 2)\n",
    "mask = x.ge(0.5) # This returns a torch.ByteTensor\n",
    "print(torch.masked_select(x, mask=mask).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.nonzero\n",
    "\n",
    "`torch.nonzero` returns a tensor with the indices of all the nonzero elements. In the example below, we apply `torch.nonzero` to a 2D tensor, and the resulting `indices` contain the row and column of the nonzero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1  0\n",
      " 0  0  0\n",
      " 0  1  1\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0  1\n",
      " 2  1\n",
      " 2  2\n",
      "[torch.LongTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[0, 1, 0], [0, 0, 0], [0, 1, 1]])\n",
    "print(x)\n",
    "indices = torch.nonzero(x)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.split\n",
    "\n",
    "`torch.split` is similar to `torch.chunk`, but instead of returning a given number of chunks, it splits the tensors in chunks of a given size. It allows for non-exact chunking. The last chunk will be smaller if the tensor size along the given `dim` is not divisible by `split_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  torch.squeeze and torch.unsqueeze\n",
    "\n",
    "`torch.squeeze` takes a tensor and returns a tensor with all the dimensions of size 1 removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0941\n",
      "  0.5801\n",
      " -0.0761\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0790\n",
      " -0.9265\n",
      "  0.5988\n",
      "[torch.FloatTensor of size 2x1x3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 3, 1)\n",
    "print(x)\n",
    "x_squeezed = torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0941  0.5801 -0.0761\n",
      "-0.0790 -0.9265  0.5988\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_squeezed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a `dim` is given, the tensor is \"squeezed\" only along that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x.squeeze(dim=1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.unsqueeze` does the opposite, but always needs a `dim` to know where to add the dimension. PyTorch only works on mini-batches. This command is useful for turning a single example into a one-example mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "  0.0941  0.5801 -0.0761\n",
      "\n",
      "(1 ,.,.) = \n",
      " -0.0790 -0.9265  0.5988\n",
      "[torch.FloatTensor of size 2x1x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x_squeezed.unsqueeze(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.stack\n",
    "\n",
    "`torch.stack` is similar to `torch.cat`, but less flexible, in that it expects all the tensors to have the same size. In the example below, `torch.cat` works, but `torch.stack` fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([4, 3]) torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1816 -0.2602 -0.0569\n",
       " 0.6215  0.4222  0.6585\n",
       "-1.3553 -0.2051 -0.2057\n",
       "-0.2135  0.5906 -0.3909\n",
       " 1.2769  0.1044 -1.4429\n",
       " 0.6632 -0.0868  0.5538\n",
       "[torch.FloatTensor of size 6x3]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x1.size(), x2.size(), x3.size())\n",
    "\n",
    "torch.cat((x1, x2), dim=0)\n",
    "\n",
    "# torch.stack((x1, x2), dim=0) # RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 2]) torch.Size([1, 2, 2]) torch.Size([1, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  1.0579  1.1103\n",
       "  1.3379 -0.7545\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  0.3428  1.2521\n",
       "  0.4236  0.1182\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "  0.3056 -0.2605\n",
       "  0.5931 -2.6714\n",
       "[torch.FloatTensor of size 3x1x2x2]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, x2, x3 = torch.chunk(torch.randn(3, 2, 2), chunks=3, dim=0)\n",
    "print(x1.size(), x2.size(), x3.size())\n",
    "torch.stack((x1, x2, x3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have retained the unit dimension in `dim=1`. We can easily get rid of it with `torch.squeeze`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.t and torch.transpose\n",
    "\n",
    "These two functions are similar, but the first one is more specialized, as it always expects a 2D tensor, and it transposes dimensions 0 and 1. `torch.transpose`, instead, takes a tensor of any size and transposes dimensions `dim1` and `dim2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2, 4)\n",
    "print(x.transpose(dim0=1, dim1=2).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.unbind\n",
    "\n",
    "`torch.unbind` removes a given dimension and returns a tuple of all slices along that dimension. The example below takes a (3, 2, 3) tensor and removes the last dimension returning a tuple of 3 (3, 2) slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      " 1.8726  0.6956\n",
      "-2.0518 -0.3820\n",
      " 0.3543  0.1863\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      "-0.2550 -0.3574\n",
      " 1.0613  3.0645\n",
      " 1.0749 -2.3235\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      " 1.9762  0.4940\n",
      "-2.5179 -0.0927\n",
      " 0.5357  0.0243\n",
      "[torch.FloatTensor of size 3x2]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2, 3)\n",
    "print(torch.unbind(x, dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting Semantics\n",
    "\n",
    "Two tensor are broadcastable if they satisfy the following properties:\n",
    "1. Each tensor has at least one dimension.\n",
    "2. When iterating along the dimension, starting from the last one, the dimension sizes must either:\n",
    "    - Be equal.\n",
    "    - One of them is equal to 1.\n",
    "    - One of them does not exist.\n",
    "\n",
    "Let's see a few cases, from the simplest to the most complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.2257 -1.4464\n",
       " 0.2828 -0.9714\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two tensors, one of which has dimension 1\n",
    "x1, x2 = torch.randn(1), torch.randn(2, 2)\n",
    "x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2  2  2  2\n",
       " 2  2  2  2\n",
       " 2  2  2  2\n",
       " 2  2  2  2\n",
       "[torch.FloatTensor of size 4x4]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two tensors with different dimension, but one of which is 1.\n",
    "x1 = torch.ones([4, 1])\n",
    "x2 = torch.ones([4])\n",
    "x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0408 -0.2068\n",
       "-0.6204  1.2971\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two tensors with the same shape (no real broadcasting here)\n",
    "x1, x2 = torch.randn(2, 2), torch.randn(2, 2)\n",
    "x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -3.2766 -1.7948\n",
       "  2.1794  1.4836\n",
       "[torch.FloatTensor of size 1x2x2]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two tensors, one of which has an extra dimension equal to one\n",
    "# In this case you can line up the dimensions as\n",
    "# (1, 2, 2)\n",
    "# (   2, 2)\n",
    "x3 = torch.randn(1, 2, 2)\n",
    "x1 + x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -2.5369 -1.7435\n",
       " -1.1534  1.0840\n",
       "\n",
       "(1 ,.,.) = \n",
       " -2.4881 -2.2367\n",
       " -1.1045  0.5908\n",
       "[torch.FloatTensor of size 2x2x2]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But the position of the 1-dimension does not matter\n",
    "x3 = torch.randn(2, 1, 2)\n",
    "x1 + x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -1.1321 -1.2815\n",
       " -0.0913  3.1672\n",
       "\n",
       "(1 ,.,.) = \n",
       " -1.8309 -2.7214\n",
       "  0.3292  1.2932\n",
       "[torch.FloatTensor of size 2x2x2]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two tensors with the same dimensions, one of which does not exist. \n",
    "# You can line up the dimensions as \n",
    "# (2, 2, 2)\n",
    "# (   2, 2)                                  \n",
    "x2 = torch.randn(2, 2, 2)\n",
    "x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When two tensors are broadcastable, the dimension of the resulting tensor is computed as follows:\n",
    "1. If the two tensors do not have the same dimensions, a 1 is prepended to the dimensions of the tensor with fewer dimensions.\n",
    "2. After this step, or if the tensors have already the same dimensions, for each dimension the final dimension is obtained as the max of the two corresponding dimensions.\n",
    "\n",
    "For example, in the case below we add one (3, 3) tensor with a (1, 3) tensor, and the result has dimension (3, 3) obtained by adding the row tensor to each row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1  2\n",
      " 3  4  5\n",
      " 6  7  8\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n",
      "\n",
      " 0  1  2\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "\n",
      "  0   2   4\n",
      "  3   5   7\n",
      "  6   8  10\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor(torch.arange(9).view(3, 3))\n",
    "print(x1)\n",
    "x2 = torch.Tensor(torch.arange(3).view(1, 3))\n",
    "print(x2)\n",
    "print(x1 + x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we consider a column tensor, we end up adding it to each column of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 1\n",
      " 2\n",
      "[torch.FloatTensor of size 3x1]\n",
      "\n",
      "\n",
      "  0   1   2\n",
      "  4   5   6\n",
      "  8   9  10\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x2 = x2.view(3, 1)\n",
    "print(x2)\n",
    "print(x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
