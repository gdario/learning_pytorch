{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LSTM in PyTorch\n",
    "\n",
    "## Building a simple vocabulary from a collection of sentences\n",
    "\n",
    "In PyTorch the input to an LSTM is expected to be a 3D tensor. Suppose we have a collection of sentences as an input. Let's start from the very beginning, showing how to go from a collection of strings to a vocabulary organized as a Python dictionary. As a simplification, all the sentences contain the same number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'had', 'breakfast', 'this', 'morning'], ['then', 'I', 'took', 'a', 'walk'], ['but', 'the', 'weather', 'was', 'bad']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "sentences = ['I had breakfast this morning'.split(' '),\n",
    "             'then I took a walk'.split(' '),\n",
    "             'but the weather was bad'.split(' ')]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a vocabulary turning each sentence into a set, and then iteratively taking the union of the sets. This will provide us with a single set with the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'but', 'weather', 'this', 'was', 'a', 'I', 'breakfast', 'then', 'the', 'took', 'walk', 'bad', 'morning'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = reduce(set.union, [set(sentence) for sentence in sentences])\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily turn this set into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had': 0, 'but': 1, 'weather': 2, 'this': 3, 'was': 4, 'a': 5, 'I': 6, 'breakfast': 7, 'then': 8, 'the': 9, 'took': 10, 'walk': 11, 'bad': 12, 'morning': 13}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {word: ix for ix, word in enumerate(list(vocabulary))}\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to turn the sentences into NumPy arrays that can be passed to the LSTM model. We start pre-allocating a numpy array of zeroes with shape (number of sentences, maximum length of a sentence, number of words in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 14)\n"
     ]
    }
   ],
   "source": [
    "sentence_max_length = max([len(sentence) for sentence in sentences])\n",
    "inputs = np.zeros((len(sentences), sentence_max_length, len(vocabulary)), dtype='float32')\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then populate this array with a one hot encoder (this would not work if we wanted to create word embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]]\n",
      "(3, 5, 14)\n"
     ]
    }
   ],
   "source": [
    "for ix_sentence in range(len(sentences)):\n",
    "    for ix_word in range(len(sentences[ix_sentence])):\n",
    "        ix_vocab = vocabulary[sentences[ix_sentence][ix_word]]\n",
    "        inputs[ix_sentence, ix_word, ix_vocab] = 1\n",
    "\n",
    "print(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the correct shape\n",
    "\n",
    "Our inputs consist in three sequences each one-hot encoded into a numpy array of size 5 x 14. We can now introduce an LSTM module that will read these sequences and store the state in a hidden layer of size 5.\n",
    "From the [documentation of the LSTM layer](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM) we see that the inputs are supposed to be of shape (seq_len, batch, input_size). Here we are providing batches of size 1, which means that we will need to reshape each input to (5, 1, 14)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.reshape(3, 1, 5, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instatiate an LSTM layer with 4 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=14, hidden_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the hidden and cell states\n",
    "\n",
    "From the [documentation of the LSTM layer](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM), LSTM returns a tuple containing the outputs and a tuple with the hidden hidden state and the cell state, and receives in input the input sequences and, optionally, the same tuple. If this is not provided, it will default to a zero initialization for both the hidden and the cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 5, 14])\n"
     ]
    }
   ],
   "source": [
    "input_var = autograd.Variable(torch.from_numpy(inputs))\n",
    "print(input_var.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the LSTM layer is actually working by assigning the output of `lstm` applied to the first sequence. Here, even if it is not necessary, we provide a manually zero-initialized tuple of hidden and cell states, to show the full syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (autograd.Variable(torch.zeros(1, 1, 4)), autograd.Variable(torch.zeros(1, 1, 4)))\n",
    "out, hidden = lstm(input_var[0], hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has shape while the hidden and cell states have shapes respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([1, 5, 4])\n",
      "Hidden sizes: torch.Size([1, 5, 4]) and torch.Size([1, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "print('Output size: {}'.format(out.size()))\n",
    "print('Hidden sizes: {0} and {1}'.format(hidden[0].size(), hidden[1].size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 5 elements in the sentence, we have the output of the 4 hidden/cell states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
