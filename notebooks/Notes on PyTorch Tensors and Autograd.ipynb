{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Introduction to Pytorch\n",
    "\n",
    "## What is PyTorch\n",
    "The core object is a `torch.Tensor`. The documentation of the `torch.Tensor` class shows that it contains a large number of methods, including methods based on  mathematics, trigonometry, linear algebra (you can calculate the eigenvalues of a tensor with `torch.eig`, as explained ([here](http://pytorch.org/docs/0.3.1/torch.html#torch.eig)).\n",
    "\n",
    "### Matrix Multiplication\n",
    "There are two functions in the `torch` module: `torch.matmul` and `torch.mm`. The latter performs a matrix multiplication of two matrices, `mat1` and `mat2`, where the dimensions must match according to the rules. The function does /not/ broadcast (see the [documentation](http://pytorch.org/docs/0.3.1/torch.html#torch.mm)). There is a similar, non-broadcasting function for matrix-vector multiplication called `torch.mv`.\n",
    "\n",
    "A more general function is `torch.matmul`, as it allows broadcasting, and can handle more general cases (vector - matrix, matrix - vector etc.). See the [documentation](http://pytorch.org/docs/0.3.1/torch.html#torch.matmul).\n",
    "\n",
    "### Other Matrix Operations\n",
    "There are functions to perform the QR factorization, the Cholesky decomposition, the SVD, to compute eigenvalues and eigenvectors of a real symmetric matrix (note, this is a different function than `torch.eig`. This one is called `torch.symeig`).\n",
    "\n",
    "### Basic Operations\n",
    "Torch tensors have types and sizes (equivalent of `dtype` and `shape`). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3, 3)\n",
    "print(x.type())\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tensor type is `torch.FloatTensor` but it can be modified by `torch.set_default_tensor_type`. To create a tensor of a different type, one must call the appropriate constructor: `torch.IntTensor`, `torch.LongTensor`, `torch.CharTensor` etc. The list of availale tensors can be found [here](http://pytorch.org/docs/master/tensors.html#torch-tensor).\n",
    "\n",
    "The number of elements in a tensor can be obtained by `torch.numel`. This can be useful when flattening (*TODO* right?). For example, let's consider a batch of 3 tensors, each of size (2, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 2, 2)\n",
    "print(x.numel())    # 12\n",
    "print(x[1:].numel()) # 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resize a tensor we don't use `reshape` but `torch.view`.\n",
    "\n",
    "Many of the basic mathematical operations are both available as class methods or as binary operators (+, -, etc). Some operations have *in place* versions, including transpositions and `copy`. These operations are all post-fixed with an `_`. There are also many numpy-like operations as `torch.linspace`, `torch.logspace`, `torch.arange`.\n",
    "\n",
    "### Indexing, Slicing, Joining, Mutating\n",
    "These operations can be found in this [section](http://pytorch.org/docs/master/torch.html#indexing-slicing-joining-mutating-ops) of the documentation. *TODO*: understand what it does.\n",
    "\n",
    "### To and From NumPy\n",
    "One can turn a NumPy array into a tensor by using `torch.from_numpy()`. One can convert a tensor `x` into a NumPy array using `x.numpy()`.\n",
    "\n",
    "### CUDA\n",
    "Tensors can be moved to the GPU with `x.cuda()`.\n",
    "\n",
    "### Autograd: Automatic Differentation\n",
    "The documentation for `torch.autograd` can be found [here](http://pytorch.org/docs/master/autograd.html#module-torch.autograd). There is an additional document on [autograd mechanics](http://pytorch.org/docs/master/notes/autograd.html#autograd-mechanics).\n",
    "\n",
    "There are two core components in `autograd`: the `Variable` ([documentation](http://pytorch.org/docs/master/autograd.html#torch.autograd.Variable)) and the `Function` ([documentation](http://pytorch.org/docs/master/autograd.html#torch.autograd.Function)).\n",
    "A `Variable` wraps around a tensor and records the operations applied to it. More precisely, a `Variable` has, with a couple of minor exceptions, the same attributes and methods of a `Tensor`, but when used in an operation, it build a computational graph on which automatic differentiation can be performed. The tensor data in a variable can be accessed via the `w.data` attribute (it's an attribute, not a method). Variables are mostly used in conjunction with gradient computations, and the `Variable` constructor has an `requires_grad` argument, but by default it is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "w = Variable(torch.randn(2, 2))\n",
    "print(w.requires_grad) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable has been created by a sub-graph that requires the gradient, it will also require it. User defined nodes must specify if they require it or not.\n",
    "\n",
    "If a variable has been created by the user its `grad` will be `None`. Such objects are called *leaf objects*. In this case the variable will have a `is_leaf` attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(w.is_leaf) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad` attribute is computed by `torch.autograd.grad` ([documentation](http://pytorch.org/docs/master/autograd.html#torch.autograd.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
