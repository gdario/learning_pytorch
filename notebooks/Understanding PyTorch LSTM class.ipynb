{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Basics of PyTorch's LSTM\n",
    "\n",
    "## Introduction to LSTMs: getting the shape right\n",
    "\n",
    "In this short notebook we try to understand the meaning of the various parameters of an LSTM. We assume we want to build a simple encoder where the input is a set of 100 observations. Each observation consists of $n_x = 15$ features and has been measured $T = 10$ times.\n",
    "\n",
    "According to the [documentation of the LSTM class](http://pytorch.org/docs/0.3.1/nn.html#lstm), the input has shape `(seq_len, batch, input_size)`. There is a `batch_first` option in the LSTM constructor. We will explore this option later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# x has shape (seq_len, batch, input_size)\n",
    "input_size = 15\n",
    "seq_len = 10\n",
    "n_observations = 100\n",
    "\n",
    "# We create a hidden layer of size 8\n",
    "hidden_size = 8\n",
    "\n",
    "# We suppose this is the output of a batch generator\n",
    "x_batch = Variable(torch.randn(seq_len, BATCH_SIZE, input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the inputs right\n",
    "\n",
    "An LSTM receive in inputs a variable containing the inputs we want to process, and a tuple with the initial values of the hidden state and the cell state. If we have only one monodirectional LSTM layer the shape of these variables must be `(1, batch, hidden_size)`. More in general, given `l` layers and `d` directions, the shape will be `(l*d, batch, hidden_size)`. Therefore, if we have one layer of bidirectional LSTM, the initial hidden and cell states must have both shape `(2, batch, hidden_size)`.\n",
    "\n",
    "## Getting the outputs right\n",
    "\n",
    "LSTM returns `output, (h_n, c_n)`, where `output` is an object of shape `(seq_len, batch, hidden_size * num_directions)` containing the hidden state for each time point from the *last layer*. This can be confusing, as the documentation refers to these as the *output features*, but they indeed are the hidden states at each time point `h_t`.\n",
    "\n",
    "The tuple `(h_n, c_n)` contains the hidden state and the cell state for the last time point. Note that these are **not** only for the last layer. Their shape, in fact, is `(num_layers * num_directions, batch, hidden_size)`. So for a two bidirectional layers model, `h_n` and `c_n` will have shape `(4, batch, hidden_size)`.\n",
    "\n",
    "## Getting the initialization right\n",
    "\n",
    "The inputs to an LSTM are the data and the initial hidden and cell states, `h_0` and `c_0`. If our input has shape `(seq_len, batch, input_size)`, what should the size of `h_0` and `c_0` be? The [documentation](http://pytorch.org/docs/0.3.1/nn.html#lstm) is clear: it should be `(num_layers * num_directions, batch, hidden_size)`. We can either initialize `h_0` and `c_0` outside of the class, or inside it. We will show an example of this latter approach. It is cleaner and more self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # IMPORTANT: self-contained initialization of the hidden state\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size)\n",
    "        self.hidden = self.initialize_hidden_states()\n",
    "        \n",
    "    def initialize_hidden_states(self):\n",
    "        # BATCH_SIZE is defined outside of the class. We could pass it as a parameter, but \n",
    "        # seems weird to make a class instance depend on the batch size.\n",
    "        return (Variable(torch.zeros(1, BATCH_SIZE, self.hidden_size)),\n",
    "                Variable(torch.zeros(1, BATCH_SIZE, self.hidden_size)))\n",
    "    \n",
    "    # Note that we don't include the hidden states among the arguments of `forward`\n",
    "    def forward(self, inputs):\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now pass our input object `x` to an instance of `MyLSTM`, together with the output of `initialize_hidden_states`, we should obtain an output of shape `(seq_len, batch, hidden_size)` since `num_directions` is one, therefore the input should have shape (10, 16, 8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 8])\n"
     ]
    }
   ],
   "source": [
    "my_lstm = MyLSTM(input_size, hidden_size)\n",
    "\n",
    "# Note: we don't need to pass the initial hidden states because they \n",
    "# are generated inside the instance\n",
    "output = my_lstm(x_batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to remember\n",
    "\n",
    "1. It easy to make mistakes when defining `__init__` or using `super(..., self)`. These are difficult to spot.\n",
    "2. Always pass a `Variable` to the LSTM, not a torch tensor.\n",
    "3. If you use the *internal initialization* of the hidden states, you don't have to pass them when creating an instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional LSTMs\n",
    "\n",
    "We can extend the class before, so that it uses a bi-directional LSTM. We just need to modify the dimension of the hidden states in the `initialize_hidden_states` function, and add the option `bidirectional=True` to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyBiLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size        \n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, bidirectional=True)\n",
    "        self.hidden = self.initialize_hidden_states()\n",
    "        \n",
    "    def initialize_hidden_states(self):\n",
    "        # Note that now the first dimension is 2 because the number of directions is two\n",
    "        return (Variable(torch.zeros(2, BATCH_SIZE, self.hidden_size)),\n",
    "                Variable(torch.zeros(2, BATCH_SIZE, self.hidden_size)))\n",
    "    \n",
    "    # Note that we don't include the hidden states among the arguments of `forward`\n",
    "    def forward(self, inputs):\n",
    "        output, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this model should be `(seq_len, batch, hidden_size * num_directions)`, *i.e.*, (10, 16, 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "my_bilstm = MyBiLSTM(input_size, hidden_size)\n",
    "output = my_bilstm(x_batch)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Working with padded sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Understanding padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
