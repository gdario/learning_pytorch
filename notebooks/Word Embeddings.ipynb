{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings in Pytorch\n",
    "\n",
    "Let's start from creating a pairs of very simple embeddings, without any training involved. The idea is to first map words in a vocabulary to integers, and then to map these integers to dense real vectors. We end up, roughly speaking, with two dictionaries: one mapping words to integers, and one mapping integers to real vectors. Let's start importing the minimum number of libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a minimalistic vocabulary consisting only of the words `hello` and `world`. We map the first one to the integer 0 and the second one to the integer 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 0, 'world': 1}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = ['hello', 'world']\n",
    "word_to_ix = {word: i for (i, word) in enumerate(vocabulary)}\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function performing the embedding is, rather appropriately, `nn.Embedding`. It requires two arguments: the first one, `num_embeddings` is the size of the vocabulary, the second, `embedding_dim`, is the dimension of the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(2, 5)\n"
     ]
    }
   ],
   "source": [
    "embeds = nn.Embedding(num_embeddings=2, embedding_dim=5)\n",
    "print(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass tensors, and more precisely variables to this function. Note that if we don't pass a list to `torch.LongTensor`, we end up with a tensor of no dimension, which would not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.8702  0.3103  1.5108  0.9291  0.0045\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fails\n",
    "# hello_var = Variable(torch.LongTensor(word_to_ix['hello']))\n",
    "\n",
    "# Succeeds\n",
    "hello_var = Variable(torch.LongTensor([word_to_ix['hello']]))\n",
    "print(hello_var)\n",
    "hello_embed = embeds(hello_var)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This embeddings haven't gone through a training phase. We can still check what is their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.5181 -0.5671  0.0890  0.9829 -0.6964\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "Variable containing:\n",
      " 0.5417\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "world_embed = embeds(Variable(torch.LongTensor([word_to_ix['world']])))\n",
    "print(world_embed)\n",
    "cosine_distance = torch.dot(hello_embed, world_embed) / (\n",
    "    torch.norm(hello_embed) * torch.norm(world_embed))\n",
    "print(cosine_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram Modeling\n",
    "\n",
    "In an n-gram model we want to predict the next word given the last n words, which we refer to as the *context*. If we consider only the last two words, we have a context of size 2. In such a case our dataset is composed of trigrams, i.e., tuples containing the last two words and the next word. More in general, given an input text, we can extract the n(+1)-grams as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['One', 'day', 'I'], 'woke'),\n",
       " (['day', 'I', 'woke'], 'up,'),\n",
       " (['I', 'woke', 'up,'], 'or'),\n",
       " (['woke', 'up,', 'or'], 'so'),\n",
       " (['up,', 'or', 'so'], 'I'),\n",
       " (['or', 'so', 'I'], 'believed,'),\n",
       " (['so', 'I', 'believed,'], 'but'),\n",
       " (['I', 'believed,', 'but'], 'everything'),\n",
       " (['believed,', 'but', 'everything'], 'I'),\n",
       " (['but', 'everything', 'I'], 'knew'),\n",
       " (['everything', 'I', 'knew'], 'was'),\n",
       " (['I', 'knew', 'was'], 'different.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_ngrams(input_text, n=2):\n",
    "    input_list = input_text.split()\n",
    "    ngram_list = []\n",
    "    for i in range(len(input_list) - n):\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            tmp.append(input_list[i + j])\n",
    "        ngram_list.append((tmp, input_list[i + j + 1]))\n",
    "    return(ngram_list)\n",
    "\n",
    "test_text = 'One day I woke up, or so I believed, but everything I knew was different.'\n",
    "extract_ngrams(test_text, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Simple N-Gram Language Modeler\n",
    "\n",
    "We define a very simple model which tries to predict the next word given the previous `context_size`. The model consists of an embedding layer followed by two fully connected layers. The first one computes the activations via a ReLU non-linearity, while the second simply returns its value which are passed to a log Softmax. Note that the embeddings are flattened before being passed to the FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, debug=False):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.debug = debug\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        original_embeds = self.embeddings(inputs)\n",
    "        embeds = original_embeds.view((1, -1))\n",
    "        if self.debug:\n",
    "            print('Before reshaping: {}'.format(original_embeds.size()))\n",
    "            print('After reshaping:  {}'.format(embeds.size()))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, 0)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take our simple text, create a vocabulary and a mapping between unique words and integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(test_text.split())\n",
    "vocabulary_size = len(vocabulary)\n",
    "word_to_ix = {w: i for i, w in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the trigrams that will be then mapped to the respective indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['One', 'day'], 'I')\n"
     ]
    }
   ],
   "source": [
    "trigrams = extract_ngrams(test_text, n=2)\n",
    "print(trigrams[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert this first trigram into an integer torch variable and see the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4\n",
      " 2\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = [word_to_ix[word] for word in trigrams[0][0]]\n",
    "context_var = Variable(torch.LongTensor(context))\n",
    "print(context_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping: torch.Size([2, 5])\n",
      "After reshaping:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_ngram = NGramLanguageModeler(vocab_size=vocab_size, embedding_dim=5, \n",
    "                                  context_size=2, debug=True)\n",
    "test_output = test_ngram(context_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output of the above command we can see that the embeddings have been flattened before being processed by the fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
