{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple CNN on CIFAR10\n",
    "\n",
    "In this notebook we build a simple CNN and apply it to the CIFAR10 dataset. We start by loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 comes with the dataset collection of PyTorch. The data are downloaded to the `data` folder. We then create a data loader for the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Creation of data loaders for training and test set.\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='../data', train=True,\n",
    "    download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=20, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='../data', train=False,\n",
    "    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=20, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a simple model consisting of two consecutive pairs of convolutional and pooling layers. More precisely, the first convolutional layer creates 6 filters with a square kernel of 5. The second layer creates 16 filters, with the same kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d (3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d (16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=120)\n",
       "  (fc2): Linear(in_features=120, out_features=84)\n",
       "  (fc3): Linear(in_features=84, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(in_features=32 * 5 * 5, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a loss function based on cross-entropy. Note that the output of the last dense layer of the model is *not* a softmax. The [documentation](http://pytorch.org/docs/master/nn.html#crossentropyloss) says: \"This criterion combines LogSoftMax and NLLLoss in one single class\". We then use SGD with momentum for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the batch size for the training and the test sets are selected when creating the data iterator. Note also that we send the `inputs` and the `labels` to the GPU before calling `Variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 2.049\n",
      "[2] loss: 1.558\n",
      "[3] loss: 1.354\n",
      "[4] loss: 1.214\n",
      "[5] loss: 1.111\n",
      "[6] loss: 1.030\n",
      "[7] loss: 0.964\n",
      "[8] loss: 0.906\n",
      "[9] loss: 0.854\n",
      "[10] loss: 0.808\n",
      "[11] loss: 0.763\n",
      "[12] loss: 0.720\n",
      "[13] loss: 0.680\n",
      "[14] loss: 0.640\n",
      "[15] loss: 0.608\n",
      "[16] loss: 0.570\n",
      "[17] loss: 0.541\n",
      "[18] loss: 0.507\n",
      "[19] loss: 0.478\n",
      "[20] loss: 0.450\n",
      "[21] loss: 0.419\n",
      "[22] loss: 0.392\n",
      "[23] loss: 0.365\n",
      "[24] loss: 0.340\n",
      "[25] loss: 0.319\n",
      "[26] loss: 0.289\n",
      "[27] loss: 0.270\n",
      "[28] loss: 0.256\n",
      "[29] loss: 0.233\n",
      "[30] loss: 0.228\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, running_loss / (i + 1)))\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a test set iterator, initialize the number of correct prediction, and compute the accuracy. Note that we must put the `images` and the `labels` on the GPU as well, as the model is on the GPU. If we don't, an exception will be raised, due to the incompatible types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.01\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for data in testloader:\n",
    "    # ipdb.set_trace()\n",
    "    images, labels = data\n",
    "    outputs = net(Variable(images.cuda()))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels.cuda()).sum()\n",
    "\n",
    "print('Accuracy: {}'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `predicted` is a tensor of size `batch_size` times 10. The `torch.max` function finds the maximum element along the specified axis (axis=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
